{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pthMSXcZazoa"
      },
      "outputs": [],
      "source": [
        "#!pip install qlearnkit['pennylane']\n",
        "#!pip install --upgrade scipy pennylane\n",
        "#!pip install pennylane\n",
        "#!pip install --upgrade numpy pennylane\n",
        "#!pip install pennylane-lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pennylane as qml\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n"
      ],
      "metadata": {
        "id": "oBmKZF_tbAxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QGRU(nn.Module):\n",
        "\n",
        "    def custom_encoding(self, inputs, wires):\n",
        "        # Apply Hadamard to each qubit to create an unbiased initial state\n",
        "        for wire in range(self.n_qubits):\n",
        "            qml.Hadamard(wires=wire)\n",
        "\n",
        "        qml.templates.AngleEmbedding(torch.sin(inputs), rotation='Y', wires=wires)\n",
        "        qml.templates.AngleEmbedding(torch.cos(inputs ** 2), rotation='Z', wires=wires)\n",
        "\n",
        "\n",
        "    def custom_entangler_layer(self, weights, wires):\n",
        "        for l in range(self.n_qlayers):\n",
        "\n",
        "            qml.CNOT(wires=[0, 1])\n",
        "            qml.CNOT(wires=[1, 2])\n",
        "            qml.CNOT(wires=[2, 3])\n",
        "            qml.CNOT(wires=[3, 0])\n",
        "            qml.CNOT(wires=[0, 2])\n",
        "            qml.CNOT(wires=[1, 3])\n",
        "            qml.CNOT(wires=[2, 0])\n",
        "            qml.CNOT(wires=[3, 1])\n",
        "\n",
        "            # Apply general rotation for each qubit\n",
        "            for i, wire in enumerate(wires):\n",
        "                qml.Rot(*weights[l, i, :], wires=wire)\n",
        "\n",
        "\n",
        "    def __init__(self,\n",
        "                input_size,\n",
        "                hidden_size,\n",
        "                n_qubits=4,\n",
        "                n_qlayers=4,\n",
        "                batch_first=True,\n",
        "                return_sequences=False,\n",
        "                return_state=False,\n",
        "                backend='default.qubit'):\n",
        "        super(QGRU, self).__init__()\n",
        "        self.n_inputs = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.concat_size = self.n_inputs + self.hidden_size\n",
        "        self.n_qubits = n_qubits\n",
        "        self.n_qlayers = n_qlayers\n",
        "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\n",
        "\n",
        "        self.batch_first = batch_first\n",
        "        self.return_sequences = return_sequences\n",
        "        self.return_state = return_state\n",
        "\n",
        "        self.dev_reset = qml.device('default.qubit', wires=range(self.n_qubits))\n",
        "        self.dev_update = qml.device('default.qubit', wires=range(self.n_qubits))\n",
        "        self.dev_new = qml.device('default.qubit', wires=range(self.n_qubits))\n",
        "\n",
        "        # Reset gate\n",
        "        def _circuit_reset(inputs, weights):\n",
        "            self.custom_encoding(inputs, wires=range(self.n_qubits))\n",
        "            self.custom_entangler_layer(weights, wires=range(self.n_qubits))\n",
        "            return [qml.expval(qml.PauliZ(wires=w)) for w in range(self.n_qubits)]\n",
        "        self.qlayer_reset = qml.QNode(_circuit_reset, self.dev_reset, interface=\"torch\")\n",
        "\n",
        "        # Update gate\n",
        "        def _circuit_update(inputs, weights):\n",
        "            self.custom_encoding(inputs, wires=range(self.n_qubits))\n",
        "            self.custom_entangler_layer(weights, wires=range(self.n_qubits))\n",
        "            return [qml.expval(qml.PauliZ(wires=w)) for w in range(self.n_qubits)]\n",
        "        self.qlayer_update = qml.QNode(_circuit_update, self.dev_update, interface=\"torch\")\n",
        "\n",
        "        # New gate\n",
        "        def _circuit_new(inputs, weights):\n",
        "            self.custom_encoding(inputs, wires=range(self.n_qubits))\n",
        "            self.custom_entangler_layer(weights, wires=range(self.n_qubits))\n",
        "            return [qml.expval(qml.PauliZ(wires=w)) for w in range(self.n_qubits)]\n",
        "        self.qlayer_new = qml.QNode(_circuit_new, self.dev_new, interface=\"torch\")\n",
        "\n",
        "        weight_shapes = {\"weights\": (self.n_qlayers, self.n_qubits, 3)}\n",
        "\n",
        "        self.clayer_in = torch.nn.Linear(self.hidden_size + self.n_inputs, self.n_qubits)\n",
        "\n",
        "        self.VQC = {\n",
        "            'reset': qml.qnn.TorchLayer(self.qlayer_reset, weight_shapes),\n",
        "            'update': qml.qnn.TorchLayer(self.qlayer_update, weight_shapes),\n",
        "            'new': qml.qnn.TorchLayer(self.qlayer_new, weight_shapes)\n",
        "        }\n",
        "        self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
        "\n",
        "    def forward(self, x, init_states=None):\n",
        "        '''\n",
        "        x.shape is (batch_size, seq_length, feature_size)\n",
        "        recurrent_activation -> sigmoid\n",
        "        activation -> tanh\n",
        "        '''\n",
        "        if self.batch_first is True:\n",
        "            batch_size, seq_length, features_size = x.size()\n",
        "        else:\n",
        "            seq_length, batch_size, features_size = x.size()\n",
        "\n",
        "        if init_states is None:\n",
        "            h_t = torch.zeros(batch_size, self.hidden_size).to(x.device)  # hidden state\n",
        "        else:\n",
        "            h_t = init_states\n",
        "\n",
        "        output_seq = []\n",
        "\n",
        "        for t in range(seq_length):\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            # Concatenate input and hidden state\n",
        "            v_t = torch.cat((h_t, x_t), dim=1)\n",
        "\n",
        "            v_t = self.clayer_in(v_t)\n",
        "\n",
        "            r_t = torch.sigmoid(self.clayer_out(self.VQC['reset'](v_t)))  # reset gate\n",
        "\n",
        "            z_t = torch.sigmoid(self.clayer_out(self.VQC['update'](v_t)))  # update gate\n",
        "\n",
        "            combined_r = r_t * h_t\n",
        "            v_t_new = torch.cat((combined_r, x_t), dim=1)\n",
        "            y_t_new = self.clayer_in(v_t_new)\n",
        "\n",
        "            h_tilde = torch.tanh(self.clayer_out(self.VQC['new'](y_t_new)))  # new gate\n",
        "\n",
        "            # Compute the new hidden state\n",
        "            h_t = z_t * h_t + (1 - z_t) * h_tilde\n",
        "\n",
        "            if self.return_sequences:\n",
        "                output_seq.append(h_t.unsqueeze(1))\n",
        "\n",
        "        if self.return_sequences:\n",
        "            output_seq = torch.cat(output_seq, dim=1)\n",
        "            return output_seq\n",
        "        else:\n",
        "            return h_t"
      ],
      "metadata": {
        "id": "aI2RVB_nbJGQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}