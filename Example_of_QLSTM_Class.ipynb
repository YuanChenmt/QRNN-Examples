{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXUoItJsZ_PV"
      },
      "outputs": [],
      "source": [
        "#!pip install qlearnkit['pennylane']\n",
        "#!pip install --upgrade scipy pennylane\n",
        "#!pip install pennylane\n",
        "#!pip install --upgrade numpy pennylane\n",
        "#!pip install pennylane-lightning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pennylane as qml\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n"
      ],
      "metadata": {
        "id": "qAjK4SDeaHw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QLSTM(nn.Module):\n",
        "\n",
        "    def custom_encoding(self, inputs, wires):\n",
        "        # Apply Hadamard to each qubit to create an unbiased initial state\n",
        "        for wire in range(self.n_qubits):\n",
        "            qml.Hadamard(wires=wire)\n",
        "\n",
        "        qml.templates.AngleEmbedding(torch.sin(inputs), rotation='Y', wires=wires)\n",
        "        qml.templates.AngleEmbedding(torch.cos(inputs ** 2), rotation='Z', wires=wires)\n",
        "\n",
        "\n",
        "    def custom_entangler_layer(self, weights, wires):\n",
        "        for l in range(self.n_qlayers):\n",
        "            # Apply CNOTs for the specified entanglement structure\n",
        "            qml.CNOT(wires=[0, 1])\n",
        "            qml.CNOT(wires=[1, 2])\n",
        "            qml.CNOT(wires=[2, 3])\n",
        "            qml.CNOT(wires=[3, 0])\n",
        "            qml.CNOT(wires=[0, 2])\n",
        "            qml.CNOT(wires=[1, 3])\n",
        "            qml.CNOT(wires=[2, 0])\n",
        "            qml.CNOT(wires=[3, 1])\n",
        "\n",
        "            # Apply general rotation for each qubit\n",
        "            for i, wire in enumerate(wires):\n",
        "                qml.Rot(*weights[l, i, :], wires=wire)\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self,\n",
        "                input_size,\n",
        "                hidden_size,\n",
        "                n_qubits=4,\n",
        "                n_qlayers=4,\n",
        "                batch_first=True,\n",
        "                return_sequences=False,\n",
        "                return_state=False,\n",
        "                backend=\"default.qubit\"):\n",
        "        super(QLSTM, self).__init__()\n",
        "        self.n_inputs = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.concat_size = self.n_inputs + self.hidden_size\n",
        "        self.n_qubits = n_qubits\n",
        "        self.n_qlayers = n_qlayers\n",
        "        self.backend = backend  # \"default.qubit\", \"qiskit.basicaer\", \"qiskit.ibm\"\n",
        "\n",
        "        self.batch_first = batch_first\n",
        "        self.return_sequences = return_sequences\n",
        "        self.return_state = return_state\n",
        "\n",
        "        self.wires_forget = list(range(self.n_qubits))\n",
        "        self.wires_input = list(range(self.n_qubits))\n",
        "        self.wires_update = list(range(self.n_qubits))\n",
        "        self.wires_output = list(range(self.n_qubits))\n",
        "        self.wires_hidden = list(range(self.n_qubits))\n",
        "        self.wires_output_pre = list(range(self.n_qubits))\n",
        "\n",
        "        self.dev_forget = qml.device('default.qubit', wires=self.wires_forget)\n",
        "        self.dev_input = qml.device('default.qubit', wires=self.wires_input)\n",
        "        self.dev_update = qml.device('default.qubit', wires=self.wires_update)\n",
        "        self.dev_output = qml.device('default.qubit', wires=self.wires_output)\n",
        "        self.dev_hidden = qml.device('default.qubit', wires=self.wires_hidden)\n",
        "        self.dev_output_pre = qml.device('default.qubit', wires=self.wires_output_pre)\n",
        "\n",
        "\n",
        "\n",
        "        def _circuit_forget(inputs, weights):\n",
        "            self.custom_encoding(inputs, wires=self.wires_forget)\n",
        "            self.custom_entangler_layer(weights, wires=self.wires_forget)\n",
        "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_forget]\n",
        "        self.qlayer_forget = qml.QNode(_circuit_forget, self.dev_forget, interface=\"torch\")\n",
        "\n",
        "        def _circuit_input(inputs, weights):\n",
        "            self.custom_encoding(inputs, wires=self.wires_input)\n",
        "            self.custom_entangler_layer(weights, wires=self.wires_input)\n",
        "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_input]\n",
        "        self.qlayer_input = qml.QNode(_circuit_input, self.dev_input, interface=\"torch\")\n",
        "\n",
        "        def _circuit_update(inputs, weights):\n",
        "            self.custom_encoding(inputs, wires=self.wires_update)\n",
        "            self.custom_entangler_layer(weights, wires=self.wires_update)\n",
        "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_update]\n",
        "        self.qlayer_update = qml.QNode(_circuit_update, self.dev_update, interface=\"torch\")\n",
        "\n",
        "        def _circuit_output(inputs, weights):\n",
        "            self.custom_encoding(inputs, wires=self.wires_output)\n",
        "            self.custom_entangler_layer(weights, wires=self.wires_output)\n",
        "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_output]\n",
        "        self.qlayer_output = qml.QNode(_circuit_output, self.dev_output, interface=\"torch\")\n",
        "\n",
        "        def _circuit_hidden(inputs, weights):\n",
        "            self.custom_encoding(inputs, wires=self.wires_hidden)\n",
        "            self.custom_entangler_layer(weights, wires=self.wires_hidden)\n",
        "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_hidden]\n",
        "        self.qlayer_hidden = qml.QNode(_circuit_hidden, self.dev_hidden, interface=\"torch\")\n",
        "\n",
        "        def _circuit_output_pre(inputs, weights):\n",
        "            self.custom_encoding(inputs, wires=self.wires_output_pre)\n",
        "            self.custom_entangler_layer(weights, wires=self.wires_output_pre)\n",
        "            return [qml.expval(qml.PauliZ(wires=w)) for w in self.wires_output_pre]\n",
        "        self.qlayer_output_pre = qml.QNode(_circuit_output_pre, self.dev_output_pre, interface=\"torch\")\n",
        "\n",
        "        weight_shapes = {\"weights\": (self.n_qlayers, self.n_qubits, 3)}\n",
        "        print(f\"weight_shapes = (n_qlayers, n_qubits, 3) = ({n_qlayers}, {n_qubits}, 3)\")\n",
        "\n",
        "        self.clayer_in = torch.nn.Linear(self.hidden_size + self.n_inputs, self.n_qubits)\n",
        "\n",
        "        self.VQC = {\n",
        "            'forget': qml.qnn.TorchLayer(self.qlayer_forget, weight_shapes),\n",
        "            'input': qml.qnn.TorchLayer(self.qlayer_input, weight_shapes),\n",
        "            'update': qml.qnn.TorchLayer(self.qlayer_update, weight_shapes),\n",
        "            'output': qml.qnn.TorchLayer(self.qlayer_output, weight_shapes),\n",
        "            'hidden': qml.qnn.TorchLayer(self.qlayer_hidden, weight_shapes),\n",
        "            'output_pre': qml.qnn.TorchLayer(self.qlayer_output_pre, weight_shapes)\n",
        "        }\n",
        "        self.clayer_out = torch.nn.Linear(self.n_qubits, self.hidden_size)\n",
        "\n",
        "\n",
        "    def forward(self, x, init_states=None):\n",
        "        '''\n",
        "        x.shape is (batch_size, seq_length, feature_size)\n",
        "        recurrent_activation -> sigmoid\n",
        "        activation -> tanh\n",
        "        '''\n",
        "        if self.batch_first is True:\n",
        "            batch_size, seq_length, features_size = x.size()\n",
        "        else:\n",
        "            seq_length, batch_size, features_size = x.size()\n",
        "\n",
        "        hidden_seq = []\n",
        "        if init_states is None:\n",
        "            h_t = torch.zeros(batch_size, self.hidden_size)  # hidden state (output)\n",
        "            c_t = torch.zeros(batch_size, self.hidden_size)  # cell state\n",
        "        else:\n",
        "\n",
        "            h_t, c_t = init_states\n",
        "            h_t = h_t[0]\n",
        "            c_t = c_t[0]\n",
        "\n",
        "        for t in range(seq_length):\n",
        "\n",
        "            x_t = x[:, t, :]\n",
        "\n",
        "            v_t = torch.cat((h_t, x_t), dim=1)\n",
        "\n",
        "            v_t = self.clayer_in(v_t)\n",
        "\n",
        "\n",
        "            f_t = torch.sigmoid(self.clayer_out(self.VQC['forget'](v_t)))\n",
        "            #print(\"Shape of forget block output:\", f_t.shape)  # forget block\n",
        "\n",
        "            i_t = torch.sigmoid(self.clayer_out(self.VQC['input'](v_t)))\n",
        "            #print(\"Shape of input block output:\", i_t.shape)   # input block\n",
        "\n",
        "            g_t = torch.tanh(self.clayer_out(self.VQC['update'](v_t)))\n",
        "            c_t = (f_t * c_t) + (i_t * g_t)\n",
        "\n",
        "            o_t = torch.sigmoid(self.clayer_out(self.VQC['output'](v_t)))\n",
        "            #print(\"Shape of output block output:\", o_t.shape) # output block\n",
        "\n",
        "            h_t = self.VQC['hidden'](o_t * torch.tanh(c_t))\n",
        "            y_tilde = self.VQC['output_pre'](o_t * torch.tanh(c_t))\n",
        "            y_t = self.clayer_out(y_tilde)\n",
        "\n",
        "            hidden_seq.append(h_t.unsqueeze(0))\n",
        "\n",
        "        hidden_seq = torch.cat(hidden_seq, dim=0)\n",
        "        hidden_seq = hidden_seq.transpose(0, 1).contiguous()\n",
        "\n",
        "        return hidden_seq, (h_t, c_t)"
      ],
      "metadata": {
        "id": "XeLZRiI8aMik"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}